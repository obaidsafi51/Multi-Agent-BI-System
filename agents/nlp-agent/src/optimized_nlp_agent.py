"""
Optimized NLP Agent with parallel processing, WebSocket connections, intelligent caching,
and query classification for maximum performance and efficiency.
"""

import asyncio
import hashlib
import json
import logging
import re
import time
import uuid
from typing import Any, Dict, List, Optional, Tuple
from datetime import datetime

from .optimized_kimi_client import OptimizedKimiClient, KimiAPIError
from .hybrid_mcp_operations_adapter import HybridMCPOperationsAdapter
# Query classifier removed - using unified processing approach
from .cache_manager import AdvancedCacheManager, CacheLevel
from .context_builder import ContextBuilder
from .models import ProcessingResult, QueryContext, QueryIntent, FinancialEntity

logger = logging.getLogger(__name__)


class OptimizedNLPAgent:
    """
    Optimized NLP Agent with advanced performance features:
    - Parallel KIMI API calls (60-70% latency reduction)
    - WebSocket persistent connections to MCP server
    - Intelligent query classification and routing
    - Multi-level semantic caching
    - Request batching and connection pooling
    - Real-time event handling and schema updates
    """
    
    def __init__(
        self,
        kimi_api_key: str,
        mcp_ws_url: str = "ws://tidb-mcp-server:8000/ws",
        agent_id: str = "nlp-agent-001",
        enable_optimizations: bool = True,
        enable_semantic_caching: bool = True,
        enable_request_batching: bool = True,
        websocket_client=None  # Optional external WebSocket client
    ):
        """
        Initialize optimized NLP agent.
        
        Args:
            kimi_api_key: KIMI API key
            mcp_ws_url: WebSocket URL for MCP server
            agent_id: Unique agent identifier
            enable_optimizations: Enable all optimization features
            enable_semantic_caching: Enable semantic similarity caching
            enable_request_batching: Enable request batching
            cache_config: Cache configuration overrides
        """
        self.agent_id = agent_id or f"nlp-agent-{uuid.uuid4().hex[:8]}"
        self.mcp_ws_url = mcp_ws_url
        self.enable_optimizations = enable_optimizations
        self.enable_semantic_caching = enable_semantic_caching
        self.enable_request_batching = enable_request_batching
        
        # Initialize optimized KIMI client with connection pooling
        # Initialize cache manager
        self.cache_manager = AdvancedCacheManager(
            l1_max_size=1000,
            l1_ttl_seconds=300
        )
        
        self.kimi_client = OptimizedKimiClient(
            api_key=kimi_api_key,
            cache_manager=self.cache_manager,
            max_connections=10  # Connection pooling
        )
        
        # WebSocket client management - support both internal and external clients
        if websocket_client is not None:
            # Use provided external WebSocket client
            self.mcp_client = websocket_client
            self.owns_websocket_client = False  # Don't own external client
            self.skip_event_handlers = False  # Can set up event handlers with external client
            logger.info(f"âœ… Using external WebSocket client: {type(websocket_client)}")
        else:
            # Will create internal client later during start()
            self.mcp_client = None
            self.owns_websocket_client = True  # Will own internal client
            self.skip_event_handlers = True  # Skip until client is created
            logger.info("ðŸ“‹ Will create internal WebSocket client during startup")
        
        # MCP operations adapter (will be set up during start())
        self.mcp_ops = None
        
        # Query classifier removed - using unified processing path
        
        # Initialize context builder
        self.context_builder = ContextBuilder()
        
        # Table name mappings - will be populated dynamically from database schema
        self.table_mappings = {}
        
        # Agent state
        self.is_running = False
        self.schema_cache = {}
        self.schema_cache_ttl = 600  # 10 minutes
        self.last_schema_update = 0
        
        # Performance metrics (simplified)
        self.metrics = {
            "total_queries": 0,
            "unified_path_queries": 0,
            "mcp_llm_calls": 0,
            "cache_hits": 0,
            "websocket_requests": 0,
            "total_latency": 0.0,
            "average_latency": 0.0,
            "optimized_single_call": 0,
            "parallel_kimi_calls": 0
        }
        
        # Event handlers
        self._setup_event_handlers()
        
        logger.info(f"Optimized NLP Agent initialized: {self.agent_id}")
        
    def _get_mcp_interface(self):
        """Get MCP interface - either mcp_ops adapter or direct client"""
        return self.mcp_ops if self.mcp_ops else self
        
    async def generate_sql(self, **kwargs):
        """Direct MCP generate_sql method for external client compatibility"""
        return await self.mcp_client.send_request("llm_generate_sql_tool", kwargs)
        
    async def get_schema_context(self, **kwargs):
        """Get schema context using backend cache instead of MCP server"""
        database_name = kwargs.get('database', 'Agentic_BI')
        database_context = {'database': database_name}
        return await self._get_cached_schema_context(database_context)
        
    async def validate_query(self, query):
        """Direct MCP validate_query method for external client compatibility"""
        return await self.mcp_client.send_request("validate_sql_query", {"query": query})


    def _setup_event_handlers(self):
        """Setup event handlers for real-time updates"""
        if hasattr(self, 'skip_event_handlers') and self.skip_event_handlers:
            logger.info("Skipping event handler setup - will be set up externally")
            return
            
        if not self.mcp_client:
            logger.warning("No MCP client available for event handler setup")
            return
            
        self.mcp_client.register_event_handler("schema_update", self._handle_schema_update)
        self.mcp_client.register_event_handler("cache_invalidation", self._handle_cache_invalidation)
        self.mcp_client.register_event_handler("server_status", self._handle_server_status)
    
    async def start(self):
        """Start the optimized NLP agent"""
        try:
            logger.info("Starting optimized NLP agent...")
            
            # Initialize cache manager
            await self.cache_manager.initialize()
            logger.info("Cache manager initialized successfully")
            
            # Set up MCP client and operations adapter
            if self.owns_websocket_client:
                # Create internal WebSocket client and hybrid adapter
                logger.info("Creating internal WebSocket client and MCP operations adapter...")
                
                # Create hybrid MCP adapter (handles both WebSocket and HTTP)
                self.mcp_ops = HybridMCPOperationsAdapter(
                    ws_url=self.mcp_ws_url,
                    http_url=self.mcp_ws_url.replace("ws://", "http://").replace("/ws", ""),
                    agent_id=self.agent_id,
                    ws_failure_threshold=3,
                    ws_retry_cooldown=60.0,
                    prefer_websocket=True
                )
                
                # Get the WebSocket client from the adapter
                self.mcp_client = self.mcp_ops.websocket_client
                
                # Set up event handlers now that we have a client
                self.skip_event_handlers = False
                self._setup_event_handlers()
                
                # Start the hybrid adapter (includes WebSocket connection)
                await self.mcp_ops.start()
                logger.info("Internal WebSocket client and MCP adapter started successfully")
                
            elif self.mcp_client is not None:
                # Using external WebSocket client - set up event handlers
                logger.info("Setting up event handlers for external WebSocket client...")
                self.skip_event_handlers = False
                self._setup_event_handlers()
                logger.info("Event handlers set up for external WebSocket client")
            else:
                raise ValueError("No WebSocket client available - either provide external client or allow internal creation")
            
            # Verify KIMI API connectivity
            if not await self.kimi_client.health_check():
                logger.warning("KIMI API health check failed - NLP parsing may be limited")
            else:
                logger.info("KIMI API health check passed")
            
            # Warm up caches
            await self._warm_up_caches()
            
            self.is_running = True
            logger.info("Optimized NLP agent started successfully")
            
        except Exception as e:
            logger.error(f"Failed to start optimized NLP agent: {e}")
            raise
    
    async def stop(self):
        """Stop the NLP agent"""
        logger.info("Stopping optimized NLP agent...")
        
        self.is_running = False
        
        # Close connections (only if we own the WebSocket client)
        if self.owns_websocket_client:
            if self.mcp_ops:
                await self.mcp_ops.stop()
            elif self.mcp_client:
                await self.mcp_client.disconnect()
        await self.kimi_client.close()
        
        logger.info("Optimized NLP agent stopped")
    
    async def process_query_optimized(
        self,
        query: str,
        user_id: str,
        session_id: str,
        context: Optional[Dict[str, Any]] = None,
        database_context: Optional[Dict[str, Any]] = None
    ) -> ProcessingResult:
        """
        Simplified unified query processing.
        Uses single optimized path for all queries - no complex routing.
        """
        start_time = time.time()
        query_id = f"q_{uuid.uuid4().hex[:8]}"
        
        try:
            logger.info(f"Processing query {query_id}: {query}")
            
            # DEBUG: Log all parameters to trace database context flow
            logger.info(f"ðŸ” DEBUG process_query_optimized parameters:")
            logger.info(f"  - query: {query}")
            logger.info(f"  - user_id: {user_id}")
            logger.info(f"  - session_id: {session_id}")
            logger.info(f"  - context: {context}")
            logger.info(f"  - database_context: {database_context}")
            
            # Database context logging and validation
            if database_context:
                logger.info(f"Using database context for query {query_id}: {database_context.get('database_name', 'unknown')}")
                if not self._validate_database_context(database_context):
                    logger.warning(f"Invalid database context for query {query_id}: {database_context}")
                    # Continue processing but log the issue
            else:
                logger.info(f"No database context provided for query {query_id}")
            
            self.metrics["total_queries"] += 1
            
            # Step 1: Check semantic cache first
            if self.enable_semantic_caching:
                cached_result = await self._check_semantic_cache(query, database_context)
                if cached_result:
                    self.metrics["cache_hits"] += 1
                    logger.info(f"Query {query_id} served from semantic cache")
                    return self._create_cached_result(query_id, cached_result, start_time)
            
            # Step 2: Use comprehensive processing (all queries get full treatment)
            result = await self._process_comprehensive_path(query, user_id, session_id, context, query_id, database_context)
            
            # Step 3: Cache result if successful
            if result.success and self.enable_semantic_caching:
                await self._cache_semantic_result(query, result, database_context)
            
            # Step 4: Update metrics
            processing_time = time.time() - start_time
            result.processing_time_ms = int(processing_time * 1000)
            self.metrics["total_latency"] += processing_time
            
            logger.info(f"Query {query_id} processed via unified_path in {processing_time:.2f}s")
            
            return result
            
        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(f"Query processing failed for {query_id}: {e}")
            
            return ProcessingResult(
                query_id=query_id,
                success=False,
                error=str(e),
                processing_time_ms=int(processing_time * 1000),
                processing_path="error"
            )
    



    

    
    async def _process_comprehensive_path(
        self,
        query: str,
        user_id: str,
        session_id: str,
        context: Optional[Dict[str, Any]],
        query_id: str,
        database_context: Optional[Dict[str, Any]] = None
    ) -> ProcessingResult:
        """
        Comprehensive path processing - now used for all queries.
        - Schema-aware intent extraction with database context
        - Complete entity extraction and ambiguity detection
        - Comprehensive context building
        - Maximum accuracy and completeness
        """
        logger.info(f"Starting comprehensive path processing for query: {query}")
        
        # Progress tracking for frontend
        progress_steps = {
            "schema_context": {"status": "pending", "message": "Loading database schema..."},
            "intent_extraction": {"status": "pending", "message": "Extracting query intent..."},
            "sql_generation": {"status": "pending", "message": "Generating SQL query..."},
            "validation_analysis": {"status": "pending", "message": "Validating and analyzing..."},
            "context_building": {"status": "pending", "message": "Building response context..."}
        }
        
        try:
            logger.debug(f"Processing {query_id} via comprehensive path")
            
            # Step 1: Get schema context (database-aware, cached for performance)
            progress_steps["schema_context"]["status"] = "in_progress"
            logger.info("ðŸ”„ [1/5] Loading database schema context...")
            
            schema_context = await self._get_cached_schema_context(database_context)
            logger.info(f"ðŸ” DEBUG: schema_context = {schema_context}")
            
            # Build dynamic table mappings from schema context
            self._build_dynamic_table_mappings(schema_context)
            
            progress_steps["schema_context"]["status"] = "completed"
            progress_steps["schema_context"]["message"] = f"Schema loaded: {schema_context.get('total_tables', 0)} tables"
            
            # Format schema for LLM context
            formatted_schema = self._format_schema_for_llm(schema_context)
            logger.info(f"ðŸ” DEBUG: formatted_schema = {formatted_schema}")
            
            # Log database context usage
            if database_context and database_context.get('database_name'):
                logger.info(f"Using database context in comprehensive processing: {database_context['database_name']}")
            
            mcp_interface = self._get_mcp_interface()
            logger.info(f"MCP ops type: {type(mcp_interface)}")
            
            # Try MCP LLM tools for schema-aware extraction, fallback to basic extraction
            try:
                # Step 2: Intent extraction
                progress_steps["intent_extraction"]["status"] = "in_progress"
                logger.info("ðŸ”„ [2/5] Extracting query intent...")
                
                # âš¡ PERFORMANCE OPTIMIZATION: Only extract intent (most critical)
                # Skip entities and ambiguities extraction to reduce 3 parallel calls to 1
                logger.info("ðŸŽ¯ Using optimized single-call intent extraction")
                intent_data = await self._extract_intent_via_mcp(query, context, schema_context)
                self.metrics["optimized_single_call"] += 1
                
                progress_steps["intent_extraction"]["status"] = "completed"
                progress_steps["intent_extraction"]["message"] = "Intent extracted successfully"
                
                # Process MCP LLM results
                intent = self._create_query_intent(intent_data)
                entities = self._extract_basic_entities(query)  # Use lightweight basic extraction
                ambiguities = []  # Skip ambiguity detection for performance
                
                logger.debug(f"Optimized extraction successful for {query_id}")
                
            except Exception as mcp_error:
                logger.warning(f"MCP LLM tools unavailable for {query_id}, using fallback extraction: {mcp_error}")
                
                # Fallback: Basic intent and entity extraction
                intent = self._create_basic_query_intent(query)
                entities = self._extract_basic_entities(query)
                ambiguities = []  # Skip ambiguity detection in fallback mode
            
            # Step 3: Generate SQL
            progress_steps["sql_generation"]["status"] = "in_progress"
            logger.info("ðŸ”„ [3/5] Generating SQL query...")
            
            mcp_interface = self._get_mcp_interface()
            sql_result = await mcp_interface.generate_sql(
                natural_language_query=query,
                schema_info=self._format_schema_for_llm(schema_context)
            )
            generated_sql = self._extract_sql_from_mcp_response(sql_result)
            
            progress_steps["sql_generation"]["status"] = "completed"
            progress_steps["sql_generation"]["message"] = "SQL query generated successfully"
            
            # âš¡ PARALLEL OPTIMIZATION: Run validation and analysis concurrently
            validation_result = None
            analysis_result = None
            
            async def safe_validate():
                try:
                    if generated_sql:
                        return await mcp_interface.validate_query(generated_sql)
                except Exception as e:
                    logger.warning(f"SQL validation failed (non-critical): {e}")
                return None
            
            async def safe_analyze():
                try:
                    return await mcp_interface.analyze_query_result(
                        query=query,
                        sql_query=generated_sql,
                        context="Optimized query analysis with lighter prompts"
                    )
                except Exception as e:
                    logger.warning(f"Query analysis failed (non-critical): {e}")
                return None
            
            # Step 4: Run validation and analysis in parallel for better performance
            progress_steps["validation_analysis"]["status"] = "in_progress"
            logger.info("ðŸ”„ [4/5] Running validation and analysis in parallel...")
            
            validation_result, analysis_result = await asyncio.gather(
                safe_validate(),
                safe_analyze(),
                return_exceptions=True
            )
            
            progress_steps["validation_analysis"]["status"] = "completed"
            progress_steps["validation_analysis"]["message"] = "Validation and analysis completed in parallel"
            logger.info("âœ… Parallel validation and analysis completed")
            
            self.metrics["websocket_requests"] += 1  # Only count SQL generation as critical
            
            # Step 5: Comprehensive context building
            progress_steps["context_building"]["status"] = "in_progress"
            logger.info("ðŸ”„ [5/5] Building comprehensive response context...")
            
            comprehensive_context = self.context_builder.build_query_context(
                query=query,
                intent=intent,
                user_context=context,
                schema_context=schema_context
            )
            
            # Add analysis insights
            comprehensive_context["analysis"] = analysis_result
            
            progress_steps["context_building"]["status"] = "completed"
            progress_steps["context_building"]["message"] = "Response context built successfully"
            logger.info("âœ… [5/5] All processing steps completed successfully!")
            
            return ProcessingResult(
                query_id=query_id,
                success=True,
                intent=intent,
                sql_query=generated_sql,
                query_context=comprehensive_context,
                processing_path="unified_path"
            )
            
        except Exception as e:
            logger.error(f"Comprehensive path processing failed for {query_id}: {e}")
            raise
    
    async def _check_semantic_cache(self, query: str, database_context: Optional[Dict[str, Any]] = None) -> Optional[Dict[str, Any]]:
        """Check semantic cache for similar query results"""
        # Create cache key that includes database context
        cache_key_data = query
        if database_context and database_context.get('database_name'):
            cache_key_data = f"{query}|db:{database_context['database_name']}"
        
        cache_key = hashlib.md5(cache_key_data.encode()).hexdigest()
        return await self.cache_manager.get("semantic", cache_key)
    
    async def _cache_semantic_result(self, query: str, result: ProcessingResult, database_context: Optional[Dict[str, Any]] = None):
        """Cache result for semantic similarity matching"""
        cache_data = {
            "intent": result.intent.model_dump() if result.intent else None,
            "sql_query": result.sql_query,
            "query_context": result.query_context,
            "processing_path": result.processing_path,
            "database_context": database_context  # Store database context with cached result
        }
        
        # Create cache key that includes database context
        cache_key_data = query
        if database_context and database_context.get('database_name'):
            cache_key_data = f"{query}|db:{database_context['database_name']}"
        
        cache_key = hashlib.md5(cache_key_data.encode()).hexdigest()
        await self.cache_manager.set("semantic", cache_key, cache_data, ttl=3600)
    
    async def _get_cached_schema_context(self, database_context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Get schema context with intelligent caching and database context awareness"""
        current_time = time.time()
        
        # Create cache key based on database context
        cache_key = "default"
        if database_context and database_context.get('database_name'):
            cache_key = database_context['database_name']
            logger.debug(f"Using database-specific schema cache key: {cache_key}")
        
        # Check if cached schema is still valid for this database
        if (self.schema_cache and 
            current_time - self.last_schema_update < self.schema_cache_ttl and
            getattr(self, 'schema_cache_key', None) == cache_key):
            logger.debug(f"Using cached schema context for database: {cache_key}")
            return self.schema_cache
        
        # ðŸš« NLP AGENT SHOULD NOT FETCH SCHEMA - Use backend's cached schema only
        logger.info(f"ï¿½ Looking for cached schema from backend for {cache_key}")
        
        # Try to get schema from backend's cache via Redis direct access
        try:
            # Direct Redis access to get backend's cached schema
            if hasattr(self, 'cache_manager') and self.cache_manager and self.cache_manager._redis_client:
                backend_cache_key = f"schema_context:{cache_key}"
                logger.info(f"ðŸ” Attempting direct Redis access for key: {backend_cache_key}")
                
                # Direct Redis get without NLP cache prefixes
                redis_value = await self.cache_manager._redis_client.get(backend_cache_key)
                if redis_value:
                    cached_schema = json.loads(redis_value)
                    logger.info(f"âœ… Using backend's cached schema for {cache_key}")
                    self.schema_cache = cached_schema
                    self.last_schema_update = current_time
                    self.schema_cache_key = cache_key
                    return cached_schema
                else:
                    logger.warning(f"ðŸ” No Redis value found for key: {backend_cache_key}")
            
            # If no cached schema found, return empty schema and log warning
            logger.warning(f"âŒ No cached schema found for {cache_key} - database must be selected first in frontend")
            logger.warning(f"ðŸ’¡ Frontend should enforce database selection before allowing queries")
            
            # Return empty schema structure
            empty_schema = {
                "databases": {},
                "total_tables": 0,
                "error": "no_schema_cached",
                "message": "Database must be selected first. Please select a database in the frontend.",
                "cache_key": cache_key
            }
            
            return empty_schema
            
        except Exception as e:
            logger.warning(f"Failed to access backend's cached schema for database {cache_key}: {e}")
            
            # Return empty schema with error information
            return {
                "databases": {}, 
                "total_tables": 0, 
                "error": "cache_access_failed",
                "message": f"Could not access cached schema: {str(e)}",
                "cache_key": cache_key
            }
    
    def _format_schema_for_llm(self, schema_context: Dict[str, Any]) -> str:
        """Format schema context for LLM consumption with detailed column information"""
        try:
            schema_lines = ["Database Schema Information:"]
            
            # Get databases with detailed table schemas
            databases = schema_context.get("databases", {})
            
            if databases:
                for db_name, db_info in databases.items():
                    schema_lines.append(f"\nDatabase: {db_name}")
                    tables = db_info.get("tables", [])
                    
                    if tables:
                        schema_lines.append(f"Tables ({len(tables)}):")
                        
                        # Handle both list and dict formats for tables
                        if isinstance(tables, list):
                            for table_schema in tables:
                                if isinstance(table_schema, dict):
                                    table_name = table_schema.get("name", "unknown")
                                    # Add table information
                                    schema_lines.append(f"  - {table_name}:")
                                    
                                    # Add column details
                                    columns = table_schema.get("columns", [])
                                    if columns:
                                        schema_lines.append("    Columns:")
                                        for column in columns:
                                            col_name = column.get("name", "unknown")
                                            col_type = column.get("data_type", "unknown")
                                            nullable = "NULL" if column.get("is_nullable", True) else "NOT NULL"
                                            schema_lines.append(f"      {col_name} ({col_type}) {nullable}")
                                    
                                    # Add primary keys if available
                                    primary_keys = table_schema.get("primary_keys", [])
                                    if primary_keys:
                                        schema_lines.append(f"    Primary Keys: {', '.join(primary_keys)}")
                                    
                                    # Add indexes if available
                                    indexes = table_schema.get("indexes", [])
                                    if indexes:
                                        index_names = [idx.get("name", "unknown") for idx in indexes if isinstance(idx, dict)]
                                        if index_names:
                                            schema_lines.append(f"    Indexes: {', '.join(index_names)}")
                        elif isinstance(tables, dict):
                            # Handle dict format where tables is a dictionary
                            for table_name, table_schema in tables.items():
                                # Get the actual table name - could be in 'name' field or use the key
                                actual_table_name = table_schema.get("name", table_name) if isinstance(table_schema, dict) else table_name
                                schema_lines.append(f"  - {actual_table_name}:")
                                
                                if isinstance(table_schema, dict):
                                    # Add column details
                                    columns = table_schema.get("columns", [])
                                    if columns:
                                        schema_lines.append("    Columns:")
                                        for column in columns:
                                            col_name = column.get("name", "unknown")
                                            col_type = column.get("data_type", "unknown")
                                            nullable = "NULL" if column.get("is_nullable", True) else "NOT NULL"
                                            schema_lines.append(f"      {col_name} ({col_type}) {nullable}")
                                    
                                    # Add primary keys if available
                                    primary_keys = table_schema.get("primary_keys", [])
                                    if primary_keys:
                                        schema_lines.append(f"    Primary Keys: {', '.join(primary_keys)}")
                                    
                                    # Add indexes if available
                                    indexes = table_schema.get("indexes", [])
                                    if indexes:
                                        index_names = [idx.get("name", "unknown") for idx in indexes if isinstance(idx, dict)]
                                        if index_names:
                                            schema_lines.append(f"    Indexes: {', '.join(index_names)}")
            else:
                # Fallback to basic format if detailed schema not available
                tables = schema_context.get("tables", [])
                if tables:
                    schema_lines.append(f"Available Tables: {', '.join(tables[:10])}")
                
                metrics = schema_context.get("metrics", [])
                if metrics:
                    schema_lines.append(f"Available Metrics: {', '.join(metrics[:20])}")
            
            # Add usage instructions for LLM
            schema_lines.append("\nIMPORTANT: Generate SQL queries using ONLY the columns listed above.")
            schema_lines.append("Do NOT assume column names that are not explicitly listed in the schema.")
            schema_lines.append("If a requested column doesn't exist, suggest the closest available column.")
            
            return "\n".join(schema_lines)
            
        except Exception as e:
            logger.error(f"Error formatting schema for LLM: {e}")
            logger.debug(f"Schema context was: {schema_context}")
            return "Schema information unavailable - please check database connection"
    
    def _extract_sql_from_mcp_response(self, mcp_response: Dict[str, Any]) -> str:
        """Extract SQL query from MCP server response and fix table names"""
        try:
            extracted_sql = ""
            
            # The MCP server returns SQL in different formats
            # Check common response fields
            
            # First, check if it's directly in 'sql' field
            if "sql" in mcp_response:
                extracted_sql = str(mcp_response["sql"]).strip()
            elif "generated_text" in mcp_response:
                # Check if it's in 'generated_text' field (LLM tool response)  
                generated_text = str(mcp_response["generated_text"])
                
                # Extract SQL from markdown code blocks (re already imported at top)
                sql_match = re.search(r'```sql\s*(.*?)\s*```', generated_text, re.DOTALL | re.IGNORECASE)
                if sql_match:
                    sql_query = sql_match.group(1).strip()
                    # Remove comments and extract just the SQL
                    lines = sql_query.split('\n')
                    sql_lines = [line for line in lines if line.strip() and not line.strip().startswith('--')]
                    extracted_sql = '\n'.join(sql_lines).strip()
                else:
                    # If no code block, try to extract SQL directly
                    # Look for SELECT, INSERT, UPDATE, DELETE statements
                    sql_keywords = r'\b(SELECT|INSERT|UPDATE|DELETE|WITH)\b'
                    if re.search(sql_keywords, generated_text, re.IGNORECASE):
                        # Clean up and return the text
                        extracted_sql = generated_text.strip()
            
            # If no extracted SQL from generated_text, check other fields
            if not extracted_sql:
                # Check if it's in 'query' field
                if "query" in mcp_response:
                    extracted_sql = str(mcp_response["query"]).strip()
                
                # Check if it's in 'result' field
                elif "result" in mcp_response:
                    result = mcp_response["result"]
                    if isinstance(result, str):
                        extracted_sql = result.strip()  
                    elif isinstance(result, dict):
                        extracted_sql = self._extract_sql_from_mcp_response(result)
            
            # Apply table name mappings to fix database schema compatibility
            if extracted_sql:
                extracted_sql = self._fix_table_names(extracted_sql)
                
            return extracted_sql
            
            # Log the response structure for debugging
            logger.warning(f"Could not extract SQL from MCP response. Response keys: {list(mcp_response.keys())}")
            logger.debug(f"MCP response: {mcp_response}")
            
            return ""
            
        except Exception as e:
            logger.error(f"Error extracting SQL from MCP response: {e}")
            logger.debug(f"MCP response was: {mcp_response}")
            return ""
    
    def _fix_table_names(self, sql_query: str) -> str:
        """
        Fix table names in SQL query to match actual database schema.
        Uses dynamic table mappings from schema context when available.
        """
        try:
            # If no table mappings are available, return the SQL as-is
            # The MCP server should handle table name validation
            if not self.table_mappings:
                logger.debug("No table mappings available - using SQL as generated by MCP server")
                return sql_query
            
            fixed_sql = sql_query
            
            # Apply dynamic table name mappings if available
            for logical_name, actual_name in self.table_mappings.items():
                # Use word boundary regex to avoid partial matches
                # Match table name in FROM, JOIN, UPDATE, INSERT INTO clauses
                patterns = [
                    rf'\bFROM\s+{logical_name}\b',
                    rf'\bJOIN\s+{logical_name}\b',
                    rf'\bUPDATE\s+{logical_name}\b',
                    rf'\bINTO\s+{logical_name}\b',
                    rf'\b{logical_name}\.', # Table.column references
                    rf'\b{logical_name}\s*$', # Table at end of line
                    rf'\b{logical_name}\s*;' # Table before semicolon
                ]
                
                for pattern in patterns:
                    fixed_sql = re.sub(pattern, 
                                     lambda m: m.group(0).replace(logical_name, actual_name),
                                     fixed_sql, flags=re.IGNORECASE)
            
            if fixed_sql != sql_query:
                logger.info(f"Applied dynamic table name mappings: {sql_query} -> {fixed_sql}")
            
            return fixed_sql
            
        except Exception as e:
            logger.error(f"Error fixing table names in SQL: {e}")
            return sql_query  # Return original on error
    
    def _build_dynamic_table_mappings(self, schema_context: Dict[str, Any]) -> None:
        """
        Build dynamic table mappings from database schema context.
        This creates mappings for common table name variations.
        """
        try:
            if not schema_context or 'databases' not in schema_context:
                return
            
            # Clear existing mappings
            self.table_mappings = {}
            
            # Extract actual table names from schema
            actual_tables = set()
            for db_name, db_info in schema_context['databases'].items():
                if isinstance(db_info, dict) and 'tables' in db_info:
                    tables = db_info['tables']
                    
                    # Handle both list and dict formats for tables
                    if isinstance(tables, list):
                        for table_info in tables:
                            if isinstance(table_info, dict) and 'name' in table_info:
                                actual_tables.add(table_info['name'].lower())
                    elif isinstance(tables, dict):
                        # This handles the case where tables is a dict
                        for table_name, table_info in tables.items():
                            if isinstance(table_info, dict):
                                # Use the key (table_name) if it exists, otherwise look for 'name' field
                                name = table_info.get('name', table_name)
                                actual_tables.add(name.lower())
            
            # Build common variations mapping to actual table names
            for table_name in actual_tables:
                # Add common variations that might be generated by LLM
                variations = []
                
                # Handle underscore/space variations
                if '_' in table_name:
                    variations.append(table_name.replace('_', ' '))
                    variations.append(table_name.replace('_', ''))
                
                # Handle common financial table variations
                if 'cash' in table_name and 'flow' in table_name:
                    variations.extend(['cash_flow', 'cashflow', 'cash flow'])
                elif 'balance' in table_name and 'sheet' in table_name:
                    variations.extend(['balance_sheet', 'balance sheet', 'balancesheet'])
                elif 'pnl' in table_name or 'profit' in table_name:
                    variations.extend(['pnl', 'profit_loss', 'profit and loss', 'pnl_statement'])
                
                # Map variations to actual table name
                for variation in variations:
                    if variation.lower() != table_name.lower():
                        self.table_mappings[variation.lower()] = table_name
            
            if self.table_mappings:
                logger.info(f"Built dynamic table mappings: {self.table_mappings}")
                
        except Exception as e:
            logger.error(f"Error building dynamic table mappings: {e}")
    
    def _build_minimal_context(
        self,
        query: str,
        intent: QueryIntent,
        context: Optional[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Build minimal context for fast path processing"""
        return {
            "query_metadata": {
                "original_query": query,
                "intent": intent.model_dump() if intent else None,
                "processing_timestamp": datetime.now().isoformat(),
                "processing_path": "fast_path"
            },
            "data_agent_context": {
                "metric_type": intent.metric_type if intent else "unknown",
                "time_period": intent.time_period if intent else "unknown",
                "aggregation_level": intent.aggregation_level if intent else "monthly"
            },
            "user_context": context or {}
        }
    
    def _create_query_intent(self, intent_data: Dict[str, Any]) -> QueryIntent:
        """Create QueryIntent from KIMI response data"""
        try:
            return QueryIntent(
                metric_type=intent_data.get("metric_type", "unknown"),
                time_period=intent_data.get("time_period", "unknown"),
                aggregation_level=intent_data.get("aggregation_level", "monthly"),
                filters=intent_data.get("filters", {}),
                comparison_periods=intent_data.get("comparison_periods", []),
                visualization_hint=intent_data.get("visualization_hint"),
                confidence_score=intent_data.get("confidence_score", 0.0)
            )
        except Exception as e:
            logger.error(f"Failed to create QueryIntent: {e}")
            return QueryIntent(
                metric_type="unknown",
                time_period="unknown",
                confidence_score=0.0
            )
    
    def _process_entities(self, entities_data: List[Dict[str, Any]]) -> List[FinancialEntity]:
        """Process entities data from KIMI"""
        entities = []
        for entity_data in entities_data:
            try:
                entity = FinancialEntity(**entity_data)
                entities.append(entity)
            except Exception as e:
                logger.warning(f"Invalid entity data: {entity_data}, error: {e}")
        return entities
    
    def _process_ambiguities(self, ambiguities_data: List[Dict[str, Any]]) -> List[str]:
        """Process ambiguities data from KIMI"""
        return [amb.get("description", "") for amb in ambiguities_data]
    
    def _create_cached_result(
        self,
        query_id: str,
        cached_data: Dict[str, Any],
        start_time: float
    ) -> ProcessingResult:
        """Create ProcessingResult from cached data"""
        processing_time = time.time() - start_time
        
        return ProcessingResult(
            query_id=query_id,
            success=True,
            intent=QueryIntent(**cached_data["intent"]) if cached_data.get("intent") else None,
            sql_query=cached_data.get("sql_query", ""),
            query_context=cached_data.get("query_context", {}),
            processing_time_ms=int(processing_time * 1000),
            processing_path="cached"
        )
    
    async def _warm_up_caches(self):
        """Warm up caches with common data"""
        try:
            logger.info("Warming up caches...")
            
            # Warm up schema cache
            await self._get_cached_schema_context()
            
            # Pre-cache common query patterns
            common_queries = [
                "show revenue this quarter",
                "profit last month",
                "cash flow this year",
                "budget variance"
            ]
            
            for query in common_queries:
                # Pre-process common queries with unified approach
                try:
                    result = await self._process_unified_path(
                        query=query,
                        user_id="warmup_user",
                        session_id="warmup_session",
                        context={}
                    )
                    logger.debug(f"Warmed up cache for query: {query}")
                except Exception as warmup_error:
                    logger.debug(f"Cache warmup failed for query '{query}': {warmup_error}")
                    continue
            
            logger.info("Cache warmup completed")
            
        except Exception as e:
            logger.warning(f"Cache warmup failed: {e}")
    
    # Event handlers
    async def _handle_schema_update(self, payload: Dict[str, Any]):
        """Handle schema update events from MCP server"""
        logger.info("Received schema update event - invalidating schema cache")
        self.schema_cache = {}
        self.last_schema_update = 0
        
        # Invalidate related cache entries
        await self.cache_manager.invalidate_by_tags(["schema", "sql"])
    
    async def _handle_cache_invalidation(self, payload: Dict[str, Any]):
        """Handle cache invalidation events"""
        tags = payload.get("tags", [])
        if tags:
            invalidated = await self.cache_manager.invalidate_by_tags(tags)
            logger.info(f"Invalidated {invalidated} cache entries for tags: {tags}")
    
    async def _handle_server_status(self, payload: Dict[str, Any]):
        """Handle server status events"""
        status = payload.get("status")
        logger.debug(f"MCP server status: {status}")
    
    async def health_check(self) -> Dict[str, Any]:
        """Comprehensive health check of optimized agent"""
        health_status = {
            "agent_id": self.agent_id,
            "status": "healthy" if self.is_running else "stopped",
            "optimizations_enabled": self.enable_optimizations,
            "components": {},
            "metrics": self.get_performance_metrics(),
            "timestamp": time.time()
        }
        
        # Check KIMI client
        try:
            kimi_healthy = await self.kimi_client.health_check()
            kimi_metrics = self.kimi_client.get_metrics()
            health_status["components"]["kimi_client"] = {
                "status": "healthy" if kimi_healthy else "unhealthy",
                "metrics": kimi_metrics
            }
        except Exception as e:
            health_status["components"]["kimi_client"] = {
                "status": "error",
                "error": str(e)
            }
        
        # Check WebSocket MCP client
        try:
            mcp_healthy = await self.mcp_client.health_check()
            mcp_metrics = self.mcp_client.get_metrics()
            health_status["components"]["mcp_client"] = {
                "status": "healthy" if mcp_healthy else "unhealthy",
                "connection": self.mcp_client.is_connected,
                "metrics": mcp_metrics
            }
        except Exception as e:
            health_status["components"]["mcp_client"] = {
                "status": "error",
                "error": str(e)
            }
        
        # Check cache manager
        cache_stats = self.cache_manager.get_stats()
        health_status["components"]["cache_manager"] = {
            "status": "healthy",
            "stats": cache_stats
        }
        
        # Overall health assessment
        component_statuses = [comp.get("status") for comp in health_status["components"].values()]
        if "error" in component_statuses:
            health_status["status"] = "degraded"
        elif "unhealthy" in component_statuses:
            health_status["status"] = "degraded"
        
        return health_status
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """Get comprehensive performance metrics"""
        avg_latency = (
            self.metrics["total_latency"] / self.metrics["total_queries"]
            if self.metrics["total_queries"] > 0 else 0
        )
        
        fast_path_ratio = (
            self.metrics["fast_path_queries"] / self.metrics["total_queries"]
            if self.metrics["total_queries"] > 0 else 0
        )
        
        cache_hit_rate = (
            self.metrics["cache_hits"] / self.metrics["total_queries"]
            if self.metrics["total_queries"] > 0 else 0
        )
        
        return {
            **self.metrics,
            "average_latency": avg_latency,
            "fast_path_ratio": fast_path_ratio,
            "cache_hit_rate": cache_hit_rate,
            "total_optimization_savings": self.metrics["optimization_savings"],
            "avg_optimization_savings": (
                self.metrics["optimization_savings"] / self.metrics["total_queries"]
                if self.metrics["total_queries"] > 0 else 0
            )
        }
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """Get cache statistics from the cache manager"""
        try:
            if hasattr(self.cache_manager, 'get_stats'):
                return self.cache_manager.get_stats()
            else:
                # Return basic cache stats if get_stats method doesn't exist
                return {
                    "cache_enabled": True,
                    "cache_type": "AdvancedCacheManager",
                    "l1_cache_size": len(getattr(self.cache_manager, '_l1_cache', {})),
                    "l1_max_size": getattr(self.cache_manager, 'l1_max_size', 1000),
                    "cache_hit_rate": self.metrics.get("cache_hits", 0) / max(self.metrics.get("total_queries", 1), 1),
                    "total_cache_hits": self.metrics.get("cache_hits", 0)
                }
        except Exception as e:
            logger.warning(f"Failed to get cache stats: {e}")
            return {
                "cache_enabled": False,
                "error": str(e)
            }

    def _create_basic_query_intent(self, query: str) -> QueryIntent:
        """Create basic query intent without KIMI API"""
        query_lower = query.lower()
        
        # Basic intent detection using keyword patterns
        metric_type = "general"
        time_period = "current"
        confidence = 0.7
        
        # Detect metric type
        if any(word in query_lower for word in ["revenue", "sales", "income"]):
            metric_type = "revenue"
        elif any(word in query_lower for word in ["cost", "expense", "spending"]):
            metric_type = "expense"
        elif any(word in query_lower for word in ["profit", "margin", "earnings"]):
            metric_type = "profit"
        elif any(word in query_lower for word in ["cash", "flow"]):
            metric_type = "cash_flow"
        elif any(word in query_lower for word in ["balance", "asset", "liability"]):
            metric_type = "balance_sheet"
        
        # Detect time period
        if any(word in query_lower for word in ["quarter", "quarterly", "q1", "q2", "q3", "q4"]):
            time_period = "quarterly"
        elif any(word in query_lower for word in ["month", "monthly"]):
            time_period = "monthly"
        elif any(word in query_lower for word in ["year", "yearly", "annual"]):
            time_period = "yearly"
        elif any(word in query_lower for word in ["week", "weekly"]):
            time_period = "weekly"
        
        return QueryIntent(
            metric_type=metric_type,
            time_period=time_period,
            confidence_score=confidence,
            filters={"fallback_mode": True}
        )
    
    def _extract_basic_entities(self, query: str) -> List[FinancialEntity]:
        """Extract basic entities without KIMI API"""
        entities = []
        query_lower = query.lower()
        
        # Basic entity patterns for financial queries
        financial_patterns = [
            ("revenue", "revenue", ["sales", "income"]),
            ("expense", "expense", ["cost", "spending"]),
            ("profit", "profit", ["earnings", "margin"]),
            ("cash_flow", "cash flow", ["cash", "liquidity"]),
            ("inventory", "inventory", ["stock", "goods"]),
            ("asset", "asset", ["resources", "property"]),
            ("liability", "liability", ["debt", "obligations"])
        ]
        
        # Extract financial entities
        for entity_type, entity_value, synonyms in financial_patterns:
            if any(synonym in query_lower for synonym in [entity_value] + synonyms):
                entities.append(FinancialEntity(
                    entity_type=entity_type,
                    entity_value=entity_value,
                    confidence_score=0.8,
                    original_text=entity_value,
                    synonyms=synonyms
                ))
        
        return entities

    def _validate_database_context(self, database_context: Dict[str, Any]) -> bool:
        """
        Validate database context for NLP agent processing.
        
        Args:
            database_context: Database context to validate
            
        Returns:
            bool: True if valid, False otherwise
        """
        if not isinstance(database_context, dict):
            logger.error("Database context must be a dictionary")
            return False
        
        # Check for required fields
        required_fields = ['database_name']
        for field in required_fields:
            if field not in database_context:
                logger.error(f"Missing required field '{field}' in database context")
                return False
            
            if not database_context[field]:
                logger.error(f"Empty value for required field '{field}' in database context")
                return False
        
        # Validate database name format
        database_name = database_context['database_name']
        if not isinstance(database_name, str) or len(database_name) == 0:
            logger.error(f"Invalid database name: {database_name}")
            return False
        
        logger.debug(f"Database context validation passed: {database_context}")
        return True

    async def _extract_intent_via_mcp(self, query: str, context: Optional[Dict[str, Any]] = None, schema_context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Extract financial intent using MCP server's LLM tools instead of direct API calls"""
        logger.info(f"ðŸŽ¯ Starting intent extraction for query: {query}")
        try:
            # Create schema-aware system prompt for intent extraction
            schema_info = ""
            if schema_context and schema_context.get("total_tables", 0) > 0:
                schema_info = f"\n\nAvailable Database Schema:\n{self._format_schema_for_llm(schema_context)}\n"
                logger.debug("Using schema-aware intent extraction")
            else:
                schema_info = "\n\nNote: Database schema not available - using general financial knowledge.\n"
                logger.warning("Schema context unavailable - using schema-less intent extraction")
            
            system_prompt = f"""You are a financial data analyst AI with access to database schema information. Extract the intent from financial queries based on the available database structure.

{schema_info}

IMPORTANT: Return ONLY a valid JSON object, no additional text or markdown formatting.

Based on the database schema above, return a JSON object with these exact fields:
- metric_type: The financial metric requested - choose from available tables/columns (revenue, profit, expense, cash_flow, balance_sheet, etc.)
- time_period: The time period (current_month, current_year, yearly, quarterly, monthly, specific dates)
- aggregation_level: How to aggregate data (daily, weekly, monthly, quarterly, yearly)
- filters: Any filters or conditions mentioned (as object)
- comparison_periods: Any comparison time periods (as array)
- visualization_hint: Suggested visualization type (bar_chart, line_chart, pie_chart, table)
- confidence_score: Confidence in the extraction (0.0-1.0)

Examples:
Query: "What is the revenue for 2024?"
Response: {{"metric_type": "revenue", "time_period": "2024", "aggregation_level": "yearly", "filters": {{"year": "2024"}}, "comparison_periods": [], "visualization_hint": "bar_chart", "confidence_score": 0.9}}

Query: "show me the total revenue for this month"
Response: {{"metric_type": "revenue", "time_period": "current_month", "aggregation_level": "monthly", "filters": {{}}, "comparison_periods": [], "visualization_hint": "bar_chart", "confidence_score": 0.9}}

Query: "show cash flow"
Response: {{"metric_type": "cash_flow", "time_period": "current", "aggregation_level": "monthly", "filters": {{}}, "comparison_periods": [], "visualization_hint": "table", "confidence_score": 0.8}}"""

            # Use MCP server's llm_generate_text_tool via WebSocket
            logger.info(f"ðŸŽ¯ Calling MCP llm_generate_text_tool for intent extraction")
            result = await self.mcp_client.send_request(
                "llm_generate_text_tool",
                {
                    "prompt": f"Extract financial intent from this query: {query}",
                    "system_prompt": system_prompt,
                    "max_tokens": 500,
                    "temperature": 0.1
                }
            )
            logger.info(f"ðŸŽ¯ MCP intent extraction response received: {result}")
            
            # Parse the generated text as JSON
            generated_text = result.get("generated_text", "{}")
            logger.debug(f"Raw MCP intent response: {generated_text}")
            try:
                # Try to extract JSON from the response (might be in markdown code blocks)
                json_match = re.search(r'```json\s*(.*?)\s*```', generated_text, re.DOTALL | re.IGNORECASE)
                if json_match:
                    json_text = json_match.group(1).strip()
                else:
                    # Try to find JSON object directly
                    json_match = re.search(r'\{.*\}', generated_text, re.DOTALL)
                    json_text = json_match.group(0) if json_match else generated_text
                
                intent_data = json.loads(json_text)
                logger.debug(f"Intent extracted via MCP: {intent_data}")
                return intent_data
            except (json.JSONDecodeError, AttributeError) as e:
                logger.warning(f"Failed to parse LLM response as JSON: {generated_text}, error: {e}")
                # Fallback to basic intent extraction
                return self._extract_basic_intent(query)
                
        except Exception as e:
            logger.warning(f"ðŸŽ¯ Intent extraction via MCP failed: {e}, using fallback")
            logger.exception("Full intent extraction error:")
            return self._extract_basic_intent(query)
    
    def _extract_basic_intent(self, query: str) -> Dict[str, Any]:
        """Fallback intent extraction without LLM"""
        query_lower = query.lower()
        
        # Basic intent patterns
        intent_data = {
            "metric_type": "general",
            "time_period": "current",
            "aggregation_level": "monthly",
            "filters": {},
            "comparison_periods": [],
            "visualization_hint": "table",
            "confidence_score": 0.6
        }
        
        # Detect metric type
        if any(word in query_lower for word in ["revenue", "sales", "income"]):
            intent_data["metric_type"] = "revenue"
        elif any(word in query_lower for word in ["cost", "expense", "spending"]):
            intent_data["metric_type"] = "expense"
        elif any(word in query_lower for word in ["profit", "margin", "earnings"]):
            intent_data["metric_type"] = "profit"
        elif any(word in query_lower for word in ["cash", "flow", "cashflow"]):
            intent_data["metric_type"] = "cash_flow"
        elif any(word in query_lower for word in ["balance", "asset", "liability"]):
            intent_data["metric_type"] = "balance_sheet"
        
        # Detect time period
        if any(word in query_lower for word in ["2024", "2023", "2022"]):
            year_match = re.search(r'\b(20\d{2})\b', query)
            if year_match:
                intent_data["time_period"] = year_match.group(1)
                intent_data["aggregation_level"] = "yearly"
                intent_data["filters"]["year"] = year_match.group(1)
        elif any(word in query_lower for word in ["quarter", "quarterly", "q1", "q2", "q3", "q4"]):
            intent_data["time_period"] = "quarterly"
            intent_data["aggregation_level"] = "quarterly"
        elif any(word in query_lower for word in ["month", "monthly"]):
            intent_data["time_period"] = "monthly"
            intent_data["aggregation_level"] = "monthly"
        
        return intent_data
    
    async def _extract_entities_via_mcp(self, query: str, context: Optional[Dict[str, Any]] = None, schema_context: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """Extract financial entities using MCP server's LLM tools"""
        try:
            system_prompt = """You are a financial data analyst AI. Extract financial entities from queries.
            
Return a JSON array of entities with these fields for each:
- entity_type: Type of entity (metric, time_period, filter, etc.)
- entity_value: The actual value/text
- confidence_score: Confidence in extraction (0.0-1.0)
- original_text: Original text from query
- synonyms: List of synonyms

Example query: "Show revenue for Q1 2024"
Example response: [
    {
        "entity_type": "metric",
        "entity_value": "revenue",
        "confidence_score": 0.9,
        "original_text": "revenue",
        "synonyms": ["sales", "income"]
    },
    {
        "entity_type": "time_period",
        "entity_value": "Q1 2024",
        "confidence_score": 0.95,
        "original_text": "Q1 2024",
        "synonyms": ["first quarter 2024"]
    }
]"""

            result = await self.mcp_client.send_request(
                "llm_generate_text_tool",
                {
                    "prompt": f"Extract financial entities from this query: {query}",
                    "system_prompt": system_prompt,
                    "max_tokens": 800,
                    "temperature": 0.1
                }
            )
            
            generated_text = result.get("text", "[]")
            try:
                entities_data = json.loads(generated_text)
                logger.debug(f"Entities extracted via MCP: {entities_data}")
                return entities_data if isinstance(entities_data, list) else []
            except json.JSONDecodeError:
                logger.warning(f"Failed to parse entities JSON: {generated_text}")
                return []
                
        except Exception as e:
            logger.warning(f"Entity extraction via MCP failed: {e}")
            return []
    
    async def _extract_ambiguities_via_mcp(self, query: str, context: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """Extract ambiguities using MCP server's LLM tools"""
        try:
            system_prompt = """You are a financial data analyst AI. Identify ambiguities in financial queries.
            
Return a JSON array of ambiguities with these fields:
- description: Description of the ambiguity
- suggestions: List of clarification suggestions
- severity: Low, Medium, or High

Example query: "Show profit last year"
Example response: [
    {
        "description": "Year not specified - could be 2023 or 2024",
        "suggestions": ["Specify exact year", "Use 'profit for 2024'"],
        "severity": "Medium"
    }
]"""

            result = await self.mcp_client.send_request(
                "llm_generate_text_tool",
                {
                    "prompt": f"Identify ambiguities in this query: {query}",
                    "system_prompt": system_prompt,
                    "max_tokens": 500,
                    "temperature": 0.2
                }
            )
            
            generated_text = result.get("text", "[]")
            try:
                ambiguities_data = json.loads(generated_text)
                logger.debug(f"Ambiguities extracted via MCP: {ambiguities_data}")
                return ambiguities_data if isinstance(ambiguities_data, list) else []
            except json.JSONDecodeError:
                logger.warning(f"Failed to parse ambiguities JSON: {generated_text}")
                return []
                
        except Exception as e:
            logger.warning(f"Ambiguity extraction via MCP failed: {e}")
            return []
