# üê≥ Docker Compose - Optimized Multi-Agent BI System
# This file creates a complete containerized environment with all agents and services

services:
  # Frontend - Next.js with TypeScript and shadcn/ui
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile.dev
      cache_from:
        - node:22-alpine
      args:
        BUILDKIT_INLINE_CACHE: 1
    ports:
      - "3000:3000"
    environment:
      - NODE_ENV=development
      # Fixed: Frontend should call backend for API operations (updated port)
      - NEXT_PUBLIC_API_URL=http://localhost:8080
      - NEXT_PUBLIC_WS_URL=ws://localhost:8080
      # Backend API for system management (same as API URL for consistency)
      - NEXT_PUBLIC_BACKEND_URL=http://localhost:8080
    # Development volumes (hot reload)
    volumes:
      - ./frontend/src:/app/src:ro
      - ./frontend/public:/app/public:ro
      - ./frontend/package.json:/app/package.json:ro
      - ./frontend/package-lock.json:/app/package-lock.json:ro
      - ./frontend/next.config.ts:/app/next.config.ts:ro
      - ./frontend/tailwind.config.ts:/app/tailwind.config.ts:ro
      - ./frontend/tsconfig.json:/app/tsconfig.json:ro
      # Preserve node_modules and .next in container
      - node_modules_cache:/app/node_modules
      - next_cache:/app/.next
    command: ["npm", "run", "dev:stable"]
    depends_on:
      backend:
        condition: service_started
      nlp-agent:
        condition: service_started
    networks:
      - ai-cfo-network
    restart: unless-stopped

  # Backend - FastAPI Gateway with MCP integration
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    ports:
      - "8080:8080"  # Backend API (changed to avoid conflict with MCP server)
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=redis://redis:6379
      - RABBITMQ_URL=amqp://guest:guest@rabbitmq:5672/
      - KIMI_API_KEY=${KIMI_API_KEY}
      - SECRET_KEY=${SECRET_KEY}
      - TIDB_HOST=${TIDB_HOST}
      - TIDB_USER=${TIDB_USER}
      - TIDB_PASSWORD=${TIDB_PASSWORD}
      - TIDB_DATABASE=${TIDB_DATABASE}
      - TIDB_PORT=${TIDB_PORT}
      # Agent Service URLs (HTTP for compatibility + WebSocket for Phase 1)
      - NLP_AGENT_URL=http://nlp-agent:8001
      - DATA_AGENT_URL=http://data-agent:8002
      - VIZ_AGENT_URL=http://viz-agent:8003
      # Phase 1: WebSocket URLs - Updated with NLP agent WebSocket server
      - NLP_AGENT_WS_URL=ws://nlp-agent:8001/ws
      - DATA_AGENT_WS_URL=ws://data-agent:8012
      - VIZ_AGENT_WS_URL=ws://viz-agent:8013
      # Phase 1: Feature flags for gradual migration - NLP agent now supports WebSocket server
      - NLP_AGENT_USE_WS=true  # NLP agent now has WebSocket SERVER + CLIENT capabilities
      - DATA_AGENT_USE_WS=true
      - VIZ_AGENT_USE_WS=true
      # Frontend & Backend URLs
      - BACKEND_URL=http://backend:8080
      - FRONTEND_URL=http://frontend:3000
      - LOCALHOST_FRONTEND_URL=http://localhost:3000
      # Fixed: Use consistent MCP server URLs
      - TIDB_MCP_SERVER_URL=http://tidb-mcp-server:8000
      - MCP_SERVER_HTTP_URL=http://tidb-mcp-server:8000
      - MCP_SERVER_WS_URL=ws://tidb-mcp-server:8000/ws
      # Enable WebSocket MCP Client for optimal performance
      - USE_WEBSOCKET_MCP=true
      - USE_MCP_CLIENT=true
      # Logging configuration
      - LOG_LEVEL=INFO
      - LOG_FORMAT=standard
      - PYTHONUNBUFFERED=1
    volumes:
      - ./backend:/app
      - ./config/logging/backend_logging.yaml:/app/logging_config.yaml:ro
    depends_on:
      redis:
        condition: service_healthy
      rabbitmq:
        condition: service_healthy
      tidb-mcp-server:
        condition: service_started
    networks:
      - ai-cfo-network
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # NLP Agent with Optimized Performance Features (Main Query Interface)
  nlp-agent:
    build:
      context: ./agents/nlp-agent
      dockerfile: Dockerfile
    ports:
      - "8001:8001"  # Main NLP API (HTTP only - WebSocket server disabled)
    environment:
      # API Keys
      - KIMI_API_KEY=${KIMI_API_KEY}
      - KIMI_API_BASE_URL=https://api.moonshot.ai/v1
      - KIMI_MODEL=${KIMI_MODEL}
      
      # Service URLs - Fixed: Use correct WebSocket port and backend URL
      - MCP_SERVER_HTTP_URL=http://tidb-mcp-server:8000
      - MCP_SERVER_WS_URL=ws://tidb-mcp-server:8000/ws
      - BACKEND_URL=http://backend:8080
      - BACKEND_WS_URL=ws://backend:8080/ws
      
      # Infrastructure
      - REDIS_URL=redis://redis:6379
      - RABBITMQ_URL=amqp://guest:guest@rabbitmq:5672/
      
      # Agent Configuration
      - AGENT_ID=nlp-agent-001
      - AGENT_TYPE=nlp
      - HOST=0.0.0.0
      - PORT=8001
      
      # WebSocket server disabled - NLP agent acts as WebSocket CLIENT only
      - ENABLE_WEBSOCKETS=true
      - WEBSOCKET_HOST=0.0.0.0
      - WEBSOCKET_PORT=8011
      
      # Performance Optimizations
      - ENABLE_ADVANCED_CACHING=true
      - ENABLE_PARALLEL_PROCESSING=true
      - CONNECTION_POOL_MIN=3
      - CONNECTION_POOL_MAX=15
      - MAX_CONCURRENT_REQUESTS=10
      
      # Enhanced Monitoring & Performance v2.2.0
      - MONITORING_ENABLED=true
      - PERFORMANCE_OPTIMIZATION=true  
      - WEBSOCKET_RELIABILITY=true
      - ANOMALY_DETECTION=true
      - EXPORT_METRICS=true
      
      # Cache Configuration
      - CACHE_TTL_SECONDS=300
      - CACHE_MAX_SIZE=1000
      - SEMANTIC_SIMILARITY_THRESHOLD=0.85
      - SEMANTIC_CACHE_SIZE=500
      - QUERY_CACHE_SIZE=200
      
      # Enhanced WebSocket Configuration
      - WS_HEARTBEAT_INTERVAL=30
      - WS_RECONNECT_DELAY=1.0
      - WS_MAX_RECONNECT_DELAY=60.0
      - WS_MAX_RECONNECT_ATTEMPTS=-1
      - WS_CONNECTION_TIMEOUT=10.0
      - WS_REQUEST_TIMEOUT=30.0
      - WS_HEALTH_CHECK_INTERVAL=60.0
      - WS_BATCH_SIZE=5
      - WS_BATCH_TIMEOUT=0.1
      - WS_CIRCUIT_BREAKER_THRESHOLD=5
      - WS_CIRCUIT_BREAKER_TIMEOUT=60.0
      
      # Logging
      - LOG_LEVEL=INFO
      - PYTHONUNBUFFERED=1
    volumes:
      - ./config/logging/nlp_agent_logging.yaml:/app/logging_config.yaml:ro
    depends_on:
      redis:
        condition: service_healthy
      rabbitmq:
        condition: service_healthy
      tidb-mcp-server:
        condition: service_started
    networks:
      - ai-cfo-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Data Agent with TiDB MCP Integration
  data-agent:
    build:
      context: ./agents/data-agent
      dockerfile: Dockerfile
    ports:
      - "8002:8002"  # HTTP API port
      - "8012:8012"  # WebSocket port for Phase 2 parallel implementation
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=redis://redis:6379
      - RABBITMQ_URL=amqp://guest:guest@rabbitmq:5672/
      - TIDB_HOST=${TIDB_HOST}
      - TIDB_USER=${TIDB_USER}
      - TIDB_PASSWORD=${TIDB_PASSWORD}
      - TIDB_DATABASE=${TIDB_DATABASE}
      - TIDB_PORT=${TIDB_PORT}
      # Fixed: Consistent MCP URLs
      - MCP_SERVER_HTTP_URL=http://tidb-mcp-server:8000
      - MCP_SERVER_WS_URL=ws://tidb-mcp-server:8000/ws
      - BACKEND_URL=http://backend:8080
      - USE_MCP_CLIENT=true
      - PORT=8002
      # WebSocket Configuration (Phase 2)
      - ENABLE_WEBSOCKETS=true
      - WEBSOCKET_PORT=8012
      - WEBSOCKET_HOST=0.0.0.0
      # Logging
      - LOG_LEVEL=INFO
      - PYTHONUNBUFFERED=1
    volumes:
      - ./config/logging/data_agent_logging.yaml:/app/logging_config.yaml:ro
    depends_on:
      redis:
        condition: service_healthy
      rabbitmq:
        condition: service_healthy
      tidb-mcp-server:
        condition: service_started
    networks:
      - ai-cfo-network
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Visualization Agent
  viz-agent:
    build:
      context: ./agents/viz-agent
      dockerfile: Dockerfile
    ports:
      - "8003:8003"  # HTTP API port
      - "8013:8013"  # WebSocket port for Phase 2 parallel implementation
    environment:
      - REDIS_URL=redis://redis:6379
      - RABBITMQ_URL=amqp://guest:guest@rabbitmq:5672/
      - BACKEND_URL=http://backend:8080
      - NLP_AGENT_URL=http://nlp-agent:8001
      - DATA_AGENT_URL=http://data-agent:8002
      - PORT=8003
      # WebSocket Configuration (Phase 2)
      - ENABLE_WEBSOCKETS=true
      - WEBSOCKET_PORT=8013
      - WEBSOCKET_HOST=0.0.0.0
      # Logging
      - LOG_LEVEL=INFO
      - PYTHONUNBUFFERED=1
    volumes:
      - ./config/logging/viz_agent_logging.yaml:/app/logging_config.yaml:ro
    depends_on:
      redis:
        condition: service_healthy
      rabbitmq:
        condition: service_healthy
      nlp-agent:
        condition: service_started
    networks:
      - ai-cfo-network
    restart: unless-stopped

  # TiDB MCP Server with WebSocket Support
  tidb-mcp-server:
    build:
      context: ./tidb-mcp-server
      dockerfile: Dockerfile
    container_name: tidb-mcp-server
    ports:
      - "8000:8000"  # HTTP API and WebSocket endpoint
    environment:
      # TiDB Connection (required - set in .env file)
      - TIDB_HOST=${TIDB_HOST}
      - TIDB_PORT=${TIDB_PORT:-4000}
      - TIDB_USER=${TIDB_USER}
      - TIDB_PASSWORD=${TIDB_PASSWORD}
      - TIDB_DATABASE=${TIDB_DATABASE}
      - TIDB_SSL_CA=${TIDB_SSL_CA:-}
      - TIDB_SSL_VERIFY_CERT=${TIDB_SSL_VERIFY_CERT:-true}
      - TIDB_SSL_VERIFY_IDENTITY=${TIDB_SSL_VERIFY_IDENTITY:-true}

      # MCP Server Configuration
      - MCP_SERVER_NAME=${MCP_SERVER_NAME:-tidb-mcp-server}
      - MCP_SERVER_VERSION=${MCP_SERVER_VERSION:-0.1.0}
      - MCP_MAX_CONNECTIONS=${MCP_MAX_CONNECTIONS:-20}
      - MCP_REQUEST_TIMEOUT=${MCP_REQUEST_TIMEOUT:-30}

      # Cache Configuration
      - CACHE_ENABLED=${CACHE_ENABLED:-true}
      - CACHE_TTL_SECONDS=${CACHE_TTL_SECONDS:-300}
      - CACHE_MAX_SIZE=${CACHE_MAX_SIZE:-1000}

      # Security Configuration
      - MAX_QUERY_TIMEOUT=${MAX_QUERY_TIMEOUT:-30}
      - MAX_SAMPLE_ROWS=${MAX_SAMPLE_ROWS:-100}
      - RATE_LIMIT_RPM=${RATE_LIMIT_RPM:-100}

      # HTTP API Configuration
      - USE_HTTP_API=${USE_HTTP_API:-true}
      - ENABLE_WEBSOCKETS=${ENABLE_WEBSOCKETS:-true}

      # LLM Configuration (using KIMI API)
      - LLM_API_KEY=${KIMI_API_KEY}  # Primary LLM API key
      - LLM_PROVIDER=kimi
      - LLM_BASE_URL=https://api.moonshot.ai/v1
      - LLM_MODEL=kimi-k2-0905-preview
      - LLM_MAX_TOKENS=4000
      - LLM_TEMPERATURE=0.7
      - LLM_TIMEOUT=30
      
      # Tools Configuration  
      - LLM_TOOLS_ENABLED=true
      - DATABASE_TOOLS_ENABLED=true
      - ANALYTICS_TOOLS_ENABLED=true
      - ENABLED_TOOLS=database,llm,analytics

      # Logging Configuration
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - LOG_FORMAT=json
      - PYTHONUNBUFFERED=1
      
      # WebSocket Configuration
      - WS_HEARTBEAT_INTERVAL=120
      - WS_CLEANUP_INTERVAL=60
    volumes:
      # Mount logs directory for persistence
      - ./tidb-mcp-server/logs:/app/logs
      # Mount logging configuration
      - ./config/logging/mcp_server_logging.yaml:/app/logging_config.yaml:ro
      # Mount SSL certificates if needed
      - ./tidb-mcp-server/certs:/app/certs:ro
    networks:
      - ai-cfo-network
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: "1.0"
        reservations:
          memory: 512M
          cpus: "0.5"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Personal Agent (Optional - can be enabled later)
  personal-agent:
    build:
      context: ./agents/personal-agent
      dockerfile: Dockerfile
    ports:
      - "8005:8005"  # Personal agent API
    environment:
      - REDIS_URL=redis://redis:6379
      - RABBITMQ_URL=amqp://guest:guest@rabbitmq:5672/
      - DATABASE_URL=${DATABASE_URL}
      - TIDB_HOST=${TIDB_HOST}
      - TIDB_USER=${TIDB_USER}
      - TIDB_PASSWORD=${TIDB_PASSWORD}
      - TIDB_DATABASE=${TIDB_DATABASE}
      - TIDB_PORT=${TIDB_PORT}
      - MCP_SERVER_HTTP_URL=http://tidb-mcp-server:8000
      - BACKEND_URL=http://backend:8080
      - PORT=8005
    depends_on:
      redis:
        condition: service_healthy
      rabbitmq:
        condition: service_healthy
      tidb-mcp-server:
        condition: service_started
    networks:
      - ai-cfo-network
    restart: unless-stopped
    profiles:
      - full  # Only start with --profile full

  # Redis for MCP Context Store and Caching
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    networks:
      - ai-cfo-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: "0.5"

  # RabbitMQ for A2A Message Broker
  rabbitmq:
    image: rabbitmq:3-management-alpine
    ports:
      - "5672:5672"   # AMQP port
      - "15672:15672" # Management UI
    environment:
      - RABBITMQ_DEFAULT_USER=guest
      - RABBITMQ_DEFAULT_PASS=guest
    volumes:
      - rabbitmq_data:/var/lib/rabbitmq
      # Mount custom configuration to suppress deprecation warnings
      - ./config/rabbitmq/rabbitmq.conf:/etc/rabbitmq/rabbitmq.conf:ro
    networks:
      - ai-cfo-network
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: "0.5"

volumes:
  redis_data:
    driver: local
  rabbitmq_data:
    driver: local
  tidb_data:
    driver: local
  # Frontend cache volumes to prevent rebuilds
  node_modules_cache:
    driver: local
  next_cache:
    driver: local

networks:
  ai-cfo-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16
